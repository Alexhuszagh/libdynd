Copyright (C) 2011-2012, Mark Wiebe
All rights reserved.
Creative Commons Attribution-ShareAlike 3.0
http://creativecommons.org/licenses/by-sa/3.0/

Dynamic ND-Array Library Design
-------------------------------

This is a library that is being developed within the spirit of NumPy
and ideas that have evolved within that community, but designed to be
comfortable in C++. It's main goals are:

* Have a flexible and high-performance data type mechanism, which can
  be extended to have new data types with POD or object semantics.
* A simple strided memory layout like NumPy, with KEEPORDER semantics
  by default.
* A design that's flexible enough to allow the data to be stored
  in CPU memory, GPU memory, or distributed across a cluster in a global
  array.
* Delayed/lazy evaluation by default - all operations return views or
  arrays with their computation deferred. As a flexible C++ library,
  this could still be used as a tool to create a non-lazy evaluation library.
* Include a compiler technology to take the deferred expression trees
  and create optimized execution plans. Either directly executing the
  expression tree, creating a compact block-based execution engine along
  the lines of numexpr, or JITting with LLVM.

Some principles which seem desirable:

1. Everything is a view. Array operations do not compute an answer,
   they merely compute the new dtype, geometry, and an expression tree.
   Since this is based on reference counting, using the library to
   implement copy-on-write semantics is possible as well.
2. There is no default "flat" perspective, any conversion between
   multi-dimensional and one-dimensional is explicit.
3. Default type conversion follows strict value-preserving semantics,
   except double -> float conversions in the precision-loss case.
4. Default overflow checking for integer math. This appears to require
   assembly-language implementations of inner loops, and precludes the
   use of SSE operations, as they set no overflow/carry flags when doing
   integer operations.

Some performance-oriented design features:

1. The dtype has all the basic necessary information in a pass-by-value
   class. For dtypes that require extended information, like structs,
   a virtual base class with additional information is added as well,
   and is reference counted which works because the extended info is
   treated as an immutable object once created.
   NOTE: After seeing Bryan's enum NEP proposal, it might be good
         to relax the immutable restriction in some way to support the
         "open"/"closed" idea.
2. The ndarray always exposes arrays that are aligned and in NBO. The only
   way to see a non-NBO array is to dig into the expression tree itself,
   behind the veil of buffering. This allows fast default operations
   which don't have to check that the data is sane every time.
3. Computations have inner loop functions that can be specialized based on
   the striding of the data in that inner loop. Important special cases
   are constant strides and strides equal to the dtype element_size. I've i
   confirmed that g++ 4.6.1 creates SSE based inner loops for the contiguous 
   versions of the arithmetic operations.

Everything Is A View
--------------------

In NumPy, some operations return views, and others return copies. In
this library, the idea is that everything is a view, either directly via
a linear striding operation, or indirectly via an expression tree.
Leaf nodes of the expression trees can either be ndarrays themselves,
or rvalue array generators, such as a constant generator or a linear range
generator.

This design choice is analogous to the shift towards using iterators
for most things in Python 3. For example, in Python 2, range(4) returns
a list [0, 1, 2, 3], whereas in Python 3 it returns a generator object
whose repr is 'range(0, 4)'.

Every operation in the system produces a view of the inputs. Suppose we
have two arrays, 'a' and 'b'. If we assign 'c = a + b', 'c' is now a view
into the sum of 'a' and 'b'. Note that 'c' itself is now an rvalue array, as
there are multiple ways an assigned value could be split up between 'a' and
'b', so such an assignment is not unique. If we take a value of 'c', for
example 'c(0,0).as<float>()', then modify 'a(0,0)', taking that value of 'c'
again will be different. The view nature of this can be collapsed by running
'c.as_lvalue()' (function subject to renaming).

Some example ndarray_expr_node subclasses:

zero_gen<dtype>, one_gen<dtype>, constant_gen<dtype, value> (rvalue):
    Generates 0, 1 or arbitrary constant values, respectively.

int_range<integer dtype, origin, step[ndim], shape[ndim]> (rvalue):
    Generates an integer range. This structure can be preserved under
    remapping to multi-dimensional data and linear indexing.

constant_multiply<dtype, A, childnode> (maybe lvalue?):
    Maps the input value x to A * x.

constant_add<dtype, A, childnode> (maybe lvalue?):
    Maps the input value x to A + x.

multiply<dtype, childnode0, childnode1> (rvalue):
    Maps the input value x, y to x * y. Similar versions of add, subtract,
    divide, etc. The childnodes must have identical shapes.

linear_map<float/complex dtype, A, B, childnode> (maybe lvalue?):
    Maps the input value x to A*x + B. A linspace is an int_range followed by
    a linear_map. If A is non-zero, this is invertible, and we could
    treat this as an lvalue array. Whether this in general is desireable needs
    some thought.

broadcast_shape<ndim, shape[ndim], axismap[childnode.ndim], childnode>
    Broadcasts the childnode to the specified shape, according to the specified
    mapping of the axes. These nodes are added when necessary for multiply-type
    nodes.

flat_to_ndim<ndim, shape[ndim], strides[ndim], childnode> (lvalue):
    Maps a one-dimensional array to a multi-dimensional array.
    result[i[0], ..., i[ndim-1]] ==
    childnode[strides[0]*i[0] + ... + strides[ndim-1]*i[ndim-1]].

linear_index<ndim, shape[ndim], base[ndim], strides[ndim], childnode> (lvalue):
    Does a linear strided indexing operation into the childnode. This doesn't
    change the number of dimensions of the childnode.

To heuristically determine where to create temporaries, the nodes should indicate
an estimate of how many source node accesses are required for each result
element. For example, a matrix multiply node for an m by n times an n by r matrix,
would indicate 2*n for each output element.

Reference Assignment vs Value Assignment
----------------------------------------

This library has two types of assignment operations, assigning by
reference and assigning by value. This is similar to NumPy, where
this distinction can trip up programmers fairly easily:

    a = np.arange(10)
    for x in np.nditer(a):
        # Oops! Wanted value assignment, not reference assignment
        x = np.sin(x)
        # Should have been this:
        # x[...] = np.sin(x)

In a C++ library, assignment is usually done by value, and the language
has lvalue and rvalue references built in. The dynamic nature of ndarray,
in particular the idea that "everything is a view", makes it much more
natural for assignment of ndarrays to be done by reference in the default
case. At the same time, it is imperative that there be a simple, convenient
way to do value assignment.

The initial design with operator= for reference assignment had a
troublesome special case that could trip up programmers. Assigning
a C++ scalar to an indexed subset of an array can lead to a
big surprise as follows:

    ndarray a = arange(10).as_strided();
    a(3) = 2;
    // Oops! Wanted value assignment, not reference assignment
    // This turns into a NOP, because a(3) produces a temporary ndarray,
    // which becomes a reference to an array containing the value 2, then
    // gets destroyed. The ndarray 'a' remains untouched, and the programmer
    // gets no signal that this happens!

To retain programmer sanity, the above should produce a compile error, an
exception, or should be a value assignment. The solution I have found is to
turn it into a compile error, by having the indexing operation return a
const ndarray. Here, we view the ndarray reference as const, not the values
it points to. Since assignment is supposed to cause the ndarray to reference
a new array, it is only defined for non-const ndarrays. The function val_assign
is defined for const ndarrays, because it is modifying the values the ndarray
points at, not the ndarray reference itself.

A more intuitive syntax for the val_assign function would be nice as well. This
has been done by introducing an object which collapses to a strided array when
its values are read, and assigns by value when being assigned to. This
works as follows:

    ndarray a = {1, 2, 3, 4, 5};
    ndarray b;

    // Compile error
    a(3) = 2;
    // This assigns to the values in 'a'
    a(3).vals() = 2;
    // This turns 'b' into a strided array containing the
    // values produced by the expression 'a+2'.
    b = (a + 2).vals();

It is worth keeping the val_assign function as well, since it has another
parameter for the assignment error checking rule. ... Or, the vals
function itself could take the assignment error checking rule as a parameter,
and the rule would be bound to this temporary value-assignment object.

There is a gotcha that the C++11 'auto' adds, which applies to any
template expression code like this.

    // Turns 'b' into a strided array with the values copied in
    ndarray b = (a + 2).vals()
    // Turns 'c' into the unspecified value assignment type that
    // was just supposed to have a short, temporary lifetime.
    auto c = (a + 2).vals()

Here's a taste of what can be weird when doing this kind of stuff:

http://stackoverflow.com/questions/9527820/prevent-expression-templates-binding-to-rvalue-references

All Data Is Aligned and in Native Byte Order
--------------------------------------------

NumPy supports misaligned data, and data in both big-endian and little-endian
formats. This library should too, but allowing this flexibility everywhere adds
complexity and degrades performance. Instead, data which is not aligned and
in NBO will be exposed in the style of a "generator array". This works nicely
with the expression tree basis for the ndarray, allowing the evaluation code
to assume the data is well-behaved when provided with a strided array, and
enable buffering otherwise.

Output Parameters
-----------------

When output parameters are used, exception safety is not guaranteed.
Since operations can fail part-way through, guaranteeing exception safety
would require making a temporary copy.  In the interests of efficiency,
this is not done. When exception safety is required, the functional-style
operations are recommended.

Label-based Indexing
--------------------

At the data array summit in Austin, a design was hammered to allow for
names of the axes and labels along each axis. There were many use-cases
people wanted for this functionality, some conflicting with each other.

I think there needs to be a big distinction between an ordered mapping,
for instance thinking of the array as a position or time instant with
associated data. For this library, I'd like to restrict the labels
to be strings only, so integer-based and label-based indexing can
live together comfortably.

The axis names and labels should themselves be one-dimensional ndarrays
of UTF8 strings.

Arrays with ndarrays as the Data Type
-------------------------------------

In J, there is an idea of a "boxed" array, which is basically having
the dtype itself consist of arrays. This is a way to support ragged
arrays, and it is natural to extend the indexing operation so that
when more indices are specified than there are dimensions, the
rest of the indices get passed on to index into the elements which
are themselves arrays.

For the dynamic ndarray library, we can treat this mechanism as a
"late binding" idea. Consider an array of strings, where the size
of the string is 0, indicating a "flexible" (this is NumPy's terminology,
perhaps a better name can be found) dtype. Behind the scenes, this
becomes a concrete dtype during each iteration step, so that the strings
themselves may be stored in a packed form along with an array of byte-ranges
to go with it. This would need corresponding API which give a dtype
that could hold all the data in a uniform fashion, which would take the maximum
length of one of the strings.

The way that having ndarray as a dtype should affect

Struct Data Types
-----------------

A data type which is a struct can be considered equivalent to an ndarray
of ndarrays, with the restriction that the subarray shapes and dtypes do
not vary across different elements. Considered in full generality, we can
imagine more ways aspects of the array are static or dynamic, For instance,
a ragged array could be imagined with a uniform dtype, but varying sizes
in the subarray.

Indirect Indexing
-----------------



Boolean Indexing
----------------

Because an ndarray has a specific shape, and the design principle was
chosen that there is no implicit "flat" perspective of an ndarray, an
ndarray "a" boolean indexed with "a>0" cannot produce an ndarray if the
shape of an array must always be fixed. Even in the one-dimensional case,
assigning a negative value to an element of 'a' which was previously
positiv ewould require that the shape of the saved result ndarray change.

To tackle this problem, likely two new facilities need to be added to
the ndarray library.

1. The possibility of an array having a selection mask.
2. The ability for one or more dimensions of the shape to be indeterminate,
   signaled with the value -1.

The selection mask can't be an NA mask like the one introduced recently
into NumPy, but rather one which corresponds loosely to the IGNORE missing
value semantics described in the NumPy missing value NEP.

In NumPy, the boolean index can apply to one, some, or all of the dimensions
of the array. In this library, I propose to restrict that to just one or
all of the dimensions. This mostly works out, but there are some serious
conflicts between facility 1 and facility 2, so they must be separated
into different functions.

To understand why conflating the two mechanisms is problematic, consider
the following C++11 code in a hypothetical C++ REPL (they do exist, for
example cling is one). In each case, the alternative interpretations of
the arrays produce vastly different results.

    :> ndarray a, b, c;
    :> a = arange(5);
    :> b = a((a % 2) == 1);
    :> cout << b << "\n";
    'b' AS SELECTION MASK) ndarray(int32, {IGNORE, 1, IGNORE, 3, IGNORE})?
    'b' AS COMPRESSED)     ndarray(int32, {1, 3})

    :> c = zeros(5, make_dtype<int>());
    :> c.val_assign(b);
    'b' AS COMPRESSED)     broadcast error: cannot broadcast shape (2) to (5)
    :> cout << c << "\n";
    'b' AS SELECTION MASK) ndarray(int32, {0, 1, 0, 3, 0})

    :> c = zeros(5, make_dtype<int>());
    :> c({true, true, false, false, false}).val_assign(b)
    :> cout << c << "\n";
    'b' AND 'c[...]' AS SELECTION MASK) ndarray(int32, {0, 1, 0, 0, 0})
    'b' AND 'c[...]' AS COMPRESSED)     ndarray(int32, {1, 3, 0, 0, 0})

I propose the selection mask be created by the function ndarray::where,
and the indeterminate dimensions be created by the indexing operator,
which is operator() in C++.  The proposed resolution looks like this:

    :> ndarray a, b, c;
    :> a = arange(5);
    :> b_ix = a((a % 2) == 1);
    :> b_wr = a.where((a % 2) == 1);
    :> cout << b_ix << "\n";
    ndarray(int32, {1, 3})
    :> cout << b_wr << "\n";
    ndarray(int32, {IGNORE, 1, IGNORE, 3, IGNORE})

    :> c = zeros(5, make_dtype<int>());
    :> c.val_assign(b_ix);
    broadcast error: cannot broadcast shape (2) to (5)
    :> c.val_assign(b_wr):
    :> cout << c << "\n";
    ndarray(int32, {0, 1, 0, 3, 0})

    :> c = zeros(5, make_dtype<int>());
    :> c({true, true, false, false, false}).val_assign(b_ix)
    :> cout << c << "\n";
    ndarray(int32, {1, 3, 0, 0, 0})
    :> c = zeros(5, make_dtype<int>());
    :> c.where({true, true, false, false, false}).val_assign(b_wr)
    ndarray(int32, {0, 1, 0, 0, 0})

One nice property of this is that for boolean indexing, the programmer
can be sure that each index entry lines up with a particular axis, instead
of having it change if the boolean index object has more than one dimension.

Rejected Boolean Indexing Ideas
-------------------------------

A bad way to handle boolean indexing would be to turn boolean indexing
into an immediate operation like in NumPy. This would create an
inconsistency in the system, and require programmers to parse expressions
in their head to determine which parts of them remain views and which
parts get baked into copies. Another reason this wouldn't be good is that
"a[a>0] + b[a>0]" is forced to evaluate the subexpression "a>0" twice. If
it remains as an expression graph, the expression evaluation code can
notice that it is a common subexpression, and only evaluate it once.

Licensing
---------

This library is licensed under the BSD 2-Clause license, except
for some parts taken from NumPy which are under the NumPy
BSD 3-Clause license.

